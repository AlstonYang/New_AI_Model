{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.3)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastWeek=4, futureWeek=1, defaultWeek=1):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureWeek-pastWeek):\n",
    "        X = np.array(train.iloc[i:i+defaultWeek])\n",
    "        X = np.append(X,train[\"CCSP\"].iloc[i+defaultWeek:i+pastWeek])\n",
    "        X_train.append(X.reshape(X.size))\n",
    "        Y_train.append(np.array(train.iloc[i+pastWeek:i+pastWeek+futureWeek][\"CCSP\"]))\n",
    "    return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler(feature_range = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \n",
    "    ## Read weekly copper price data\n",
    "    path = \"WeeklyFinalData.csv\"\n",
    "    data = read(path)\n",
    "    \n",
    "    date = data[\"Date\"]\n",
    "    data.drop(\"Date\", axis=1, inplace=True)\n",
    "    \n",
    "    ## Add time lag (pastWeek=4, futureWeek=1)\n",
    "    x_data, y_data = buildTrain(data)\n",
    "    \n",
    "    ## Data split\n",
    "    x_train = x_data[0:int(x_data.shape[0]*0.8)]\n",
    "    x_test = x_data[int(x_data.shape[0]*0.8):]\n",
    "    \n",
    "    y_train = y_data[0:int(y_data.shape[0]*0.8)]\n",
    "    y_test = y_data[int(y_data.shape[0]*0.8):]\n",
    "    \n",
    "    ## Normalize\n",
    "    x_train_scaled = sc.fit_transform(x_train)\n",
    "    x_test_scaled = sc.transform(x_test)\n",
    "    \n",
    "    y_train_scaled = sc.fit_transform(y_train)\n",
    "    y_test_scaled = sc.transform(y_test)\n",
    "    \n",
    "    ## Other information\n",
    "    nb_output = 1\n",
    "    batch_size = 10\n",
    "    input_shape = (data.shape[1],)\n",
    "    \n",
    "    return (nb_output, batch_size, input_shape, x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network_first():\n",
    "    \n",
    "    \n",
    "    def __init__(self, nb_neuro):\n",
    "        nb_output, batch_size, input_shape, x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled = get_data()\n",
    "\n",
    "        # Stop criteria - threshold\n",
    "        self.threshold_for_error = 1e-2\n",
    "        self.threshold_for_lr = 1e-4\n",
    "\n",
    "        # Input data\n",
    "        self.x = tf.convert_to_tensor(x_train_scaled, np.float32)\n",
    "        self.y = tf.convert_to_tensor(y_train_scaled, np.float32)\n",
    "\n",
    "        # Learning rate\n",
    "        self.learning_rate = 1e-2\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = tf.optimizers.SGD(self.learning_rate)\n",
    "\n",
    "        # Hidden layer I\n",
    "        self.n_neurons_in_h1 = nb_neuro\n",
    "        self.W1 = tf.Variable(tf.random.truncated_normal([self.x.shape[1], self.n_neurons_in_h1], mean=0, stddev=1), name='weights1')\n",
    "        self.b1 = tf.Variable(tf.random.truncated_normal([self.n_neurons_in_h1], mean=0, stddev=1), name = \"biases1\")\n",
    "\n",
    "        # Output layer\n",
    "        self.Wo = tf.Variable(tf.random.truncated_normal([self.n_neurons_in_h1, self.y.shape[1]], mean=0, stddev=1), name='weightsOut')\n",
    "        self.bo = tf.Variable(tf.random.truncated_normal([self.y.shape[1]], mean=0, stddev=1), name='biasesOut')\n",
    "\n",
    "#     loss_list = np.array([])\n",
    "#     predicted_values = np.array([])\n",
    "\n",
    "    # forward operation\n",
    "    def forward(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            y1 = tf.nn.relu((tf.matmul(self.x, self.W1)+self.b1), name='activationLayer1')\n",
    "            yo = (tf.matmul(y1,self.Wo)+self.bo)\n",
    "            \n",
    "            # performance measure\n",
    "            diff = yo-self.y\n",
    "            loss = tf.reduce_mean(diff**2)\n",
    "    \n",
    "        return(yo, loss, tape)\n",
    "    \n",
    "    # backward operation\n",
    "    def backward(self,tape,loss):\n",
    "        self.optimizer = tf.optimizers.SGD(self.learning_rate)\n",
    "        gradients = tape.gradient(loss, [self.W1, self.Wo, self.b1, self.bo])\n",
    "        self.optimizer.apply_gradients(zip(gradients, [self.W1, self.Wo, self.b1, self.bo]))\n",
    "        \n",
    "    \n",
    "    # tunning the parameter\n",
    "    def tuning(self, epoch):\n",
    "        \n",
    "        for i in range(1,epoch+1):\n",
    "            \n",
    "            yo, loss, tape = self.forward()\n",
    "            \n",
    "            if tf.reduce_all(tf.math.abs(yo-self.y) <= self.threshold_for_error):\n",
    "                print(\"Acceptable SLFN\")\n",
    "                return (self.W1, self.b1, self.Wo, self.bo)\n",
    "\n",
    "            else:\n",
    "                    \n",
    "                # tuning and check the loss performance of the next step\n",
    "                self.backward(tape,loss)\n",
    "\n",
    "                # Identify whether the current adjustment times are greater than the specified times\n",
    "                if i >= epoch:\n",
    "\n",
    "                    # Return the parameter of the current model\n",
    "                    print(\"Unacceptable SLFN (i >= specific number of times)\")\n",
    "                    return (self.W1, self.b1, self.Wo, self.bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unacceptable SLFN (i >= specific number of times)\n"
     ]
    }
   ],
   "source": [
    "# print(build_algorithm(10))\n",
    "network = network_first(32)\n",
    "W1, b1, Wo, bo = network.tuning(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network_second():\n",
    "    \n",
    "    \n",
    "    def __init__(self, nb_neuro):\n",
    "        nb_output, batch_size, input_shape, x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled = get_data()\n",
    "\n",
    "        # Stop criteria - threshold\n",
    "        self.threshold_for_error = 1e-2\n",
    "        self.threshold_for_lr = 1e-4\n",
    "\n",
    "        # Input data\n",
    "        self.x = tf.convert_to_tensor(x_train_scaled, np.float32)\n",
    "        self.y = tf.convert_to_tensor(y_train_scaled, np.float32)\n",
    "\n",
    "        # Learning rate\n",
    "        self.learning_rate = 1e-2\n",
    "        \n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = tf.optimizers.SGD(self.learning_rate)\n",
    "\n",
    "        # Hidden layer I\n",
    "        self.n_neurons_in_h1 = nb_neuro\n",
    "        self.W1 = tf.Variable(tf.random.truncated_normal([self.x.shape[1], self.n_neurons_in_h1], mean=0, stddev=1), name='weights1')\n",
    "        self.b1 = tf.Variable(tf.random.truncated_normal([self.n_neurons_in_h1], mean=0, stddev=1), name = \"biases1\")\n",
    "\n",
    "        # Output layer\n",
    "        self.Wo = tf.Variable(tf.random.truncated_normal([self.n_neurons_in_h1, self.y.shape[1]], mean=0, stddev=1), name='weightsOut')\n",
    "        self.bo = tf.Variable(tf.random.truncated_normal([self.y.shape[1]], mean=0, stddev=1), name='biasesOut')\n",
    "\n",
    "#     loss_list = np.array([])\n",
    "#     predicted_values = np.array([])\n",
    "\n",
    "    # forward operation\n",
    "    def forward(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            y1 = tf.nn.relu((tf.matmul(self.x, self.W1)+self.b1), name='activationLayer1')\n",
    "            yo = (tf.matmul(y1,self.Wo)+self.bo)\n",
    "            \n",
    "            # performance measure\n",
    "            diff = yo-self.y\n",
    "            loss = tf.reduce_mean(diff**2)\n",
    "    \n",
    "        return(yo, loss, tape)\n",
    "    \n",
    "    # backward operation\n",
    "    def backward(self,tape,loss):\n",
    "        self.optimizer = tf.optimizers.SGD(self.learning_rate)\n",
    "        gradients = tape.gradient(loss, [self.W1, self.Wo, self.b1, self.bo])\n",
    "        self.optimizer.apply_gradients(zip(gradients, [self.W1, self.Wo, self.b1, self.bo]))\n",
    "        \n",
    "    \n",
    "    # tunning the parameter\n",
    "    def tuning(self, epoch):\n",
    "        \n",
    "        for i in range(1,epoch+1):\n",
    "            \n",
    "            yo, loss, tape = self.forward()\n",
    "            \n",
    "            if tf.reduce_all(tf.math.abs(yo-self.y) <= self.threshold_for_error):\n",
    "                print(\"Acceptable SLFN\")\n",
    "                return (self.W1, self.b1, self.Wo, self.bo)\n",
    "\n",
    "            else:\n",
    "                while True:\n",
    "                    \n",
    "                    # Save the current papameter\n",
    "                    W1_pre = self.W1\n",
    "                    Wo_pre = self.Wo\n",
    "                    b1_pre = self.b1\n",
    "                    bo_pre = self.bo\n",
    "                    loss_pre = loss\n",
    "\n",
    "                    # tuning and check the loss performance of the next step\n",
    "                    self.backward(tape,loss)\n",
    "                    yo, loss, tape = self.forward()\n",
    "                    \n",
    "                    # Confirm whether the adjusted loss value is smaller than the current one\n",
    "                    if loss < loss_pre:\n",
    "                        \n",
    "                        # Multiply the learning rate by 1.2\n",
    "                        self.learning_rate *= 1.2\n",
    "\n",
    "                        # Identify whether the current adjustment times are greater than the specified times\n",
    "                        if i >= epoch:\n",
    "                            \n",
    "                            # Return the parameter of the current model\n",
    "                            print(\"Unacceptable SLFN (i >= specific number of times)\")\n",
    "                            return (self.W1, self.b1, self.Wo, self.bo)\n",
    "                        \n",
    "                        # On the contrary, it jumps out of the loop of adjusting the learning rate\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    # On the contrary, reduce the learning rate\n",
    "                    else:\n",
    "                        \n",
    "                        # Identify whether the current learning rate is less than the threshold\n",
    "                        if self.learning_rate < self.threshold_for_lr:\n",
    "                            \n",
    "                            # If true, return the current model parameters\n",
    "                            print(\"Unacceptable SLFN (learning_rate < threshold)\")\n",
    "                            return (self.W1, self.b1, self.Wo, self.bo)\n",
    "                        \n",
    "                        # On the contrary, maintain the original parameter and adjust the learning rate\n",
    "                        else:\n",
    "                            self.W1 = W1_pre\n",
    "                            self.Wo = Wo_pre\n",
    "                            self.b1 = b1_pre\n",
    "                            self.bo = bo_pre\n",
    "                            self.learning_rate *= 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unacceptable SLFN (i >= specific number of times)\n"
     ]
    }
   ],
   "source": [
    "network = network_second(32)\n",
    "W1, b1, Wo, bo = network.tuning(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network_third():\n",
    "    \n",
    "    \n",
    "    def __init__(self, nb_neuro):\n",
    "        nb_output, batch_size, input_shape, x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled = get_data()\n",
    "\n",
    "        # Stop criteria - threshold\n",
    "        self.threshold_for_error = 1e-2*2\n",
    "        self.threshold_for_lr = 1e-2\n",
    "\n",
    "        # Input data\n",
    "        self.x = tf.convert_to_tensor(x_train_scaled, np.float32)\n",
    "        self.y = tf.convert_to_tensor(y_train_scaled, np.float32)\n",
    "\n",
    "        # Learning rate\n",
    "        self.learning_rate = 1e-2\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = tf.optimizers.SGD(self.learning_rate)\n",
    "\n",
    "        # Hidden layer I\n",
    "        self.n_neurons_in_h1 = nb_neuro\n",
    "        self.W1 = tf.Variable(tf.random.truncated_normal([self.x.shape[1], self.n_neurons_in_h1], mean=0, stddev=1), name='weights1')\n",
    "        self.b1 = tf.Variable(tf.random.truncated_normal([self.n_neurons_in_h1], mean=0, stddev=1), name = \"biases1\")\n",
    "\n",
    "        # Output layer\n",
    "        self.Wo = tf.Variable(tf.random.truncated_normal([self.n_neurons_in_h1, self.y.shape[1]], mean=0, stddev=1), name='weightsOut')\n",
    "        self.bo = tf.Variable(tf.random.truncated_normal([self.y.shape[1]], mean=0, stddev=1), name='biasesOut')\n",
    "\n",
    "#     loss_list = np.array([])\n",
    "#     predicted_values = np.array([])\n",
    "\n",
    "    # forward operation\n",
    "    def forward(self):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            y1 = tf.nn.relu((tf.matmul(self.x, self.W1)+self.b1), name='activationLayer1')\n",
    "            yo = (tf.matmul(y1,self.Wo)+self.bo)\n",
    "            \n",
    "            # performance measure\n",
    "            diff = yo-self.y\n",
    "            loss = tf.reduce_mean(diff**2)\n",
    "    \n",
    "        return(yo, loss, tape)\n",
    "    \n",
    "    # backward operation\n",
    "    def backward(self,tape,loss):\n",
    "        \n",
    "        self.optimizer = tf.optimizers.SGD(self.learning_rate)\n",
    "        gradients = tape.gradient(loss, [self.W1, self.Wo, self.b1, self.bo])\n",
    "        self.optimizer.apply_gradients(zip(gradients, [self.W1, self.Wo, self.b1, self.bo]))\n",
    "        \n",
    "    \n",
    "    # tunning the parameter\n",
    "    def tuning(self):\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            yo, loss, tape = self.forward()\n",
    "            \n",
    "            if tf.reduce_all(tf.math.abs(yo-self.y) <= self.threshold_for_error):\n",
    "                print(\"Acceptable SLFN\")\n",
    "                return (self.W1, self.b1, self.Wo, self.bo)\n",
    "\n",
    "            else:\n",
    "                while True:\n",
    "                    \n",
    "                    # Save the current papameter\n",
    "                    W1_pre = self.W1\n",
    "                    Wo_pre = self.Wo\n",
    "                    b1_pre = self.b1\n",
    "                    bo_pre = self.bo\n",
    "                    loss_pre = loss\n",
    "\n",
    "                    # tuning and check the loss performance of the next step\n",
    "                    self.backward(tape,loss)\n",
    "                    yo, loss, tape = self.forward()\n",
    "                    \n",
    "                    # Confirm whether the adjusted loss value is smaller than the current one\n",
    "                    if loss < loss_pre:\n",
    "                        \n",
    "                        # Multiply the learning rate by 1.2\n",
    "                        self.learning_rate *= 1.2\n",
    "                        break\n",
    "\n",
    "                    # On the contrary, reduce the learning rate\n",
    "                    else:\n",
    "                        \n",
    "                        # Identify whether the current learning rate is less than the threshold\n",
    "                        if self.learning_rate < self.threshold_for_lr:\n",
    "                            \n",
    "                            # If true, return the current model parameters\n",
    "                            print(\"Unacceptable SLFN (learning_rate < threshold)\")\n",
    "                            return (W1, b1, Wo, bo)\n",
    "                        \n",
    "                        # On the contrary, maintain the original parameter and adjust the learning rate\n",
    "                        else:\n",
    "                            self.W1 = W1_pre\n",
    "                            self.Wo = Wo_pre\n",
    "                            self.b1 = b1_pre\n",
    "                            self.bo = bo_pre\n",
    "                            self.learning_rate *= 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unacceptable SLFN (learning_rate < threshold)\n"
     ]
    }
   ],
   "source": [
    "network = network_third(32)\n",
    "W1, b1, Wo, bo = network.tuning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
